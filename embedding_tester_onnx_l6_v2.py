import os
import time
import numpy as np
import onnxruntime as ort
from transformers import AutoTokenizer

# –ü—É—Ç–∏ –∫ ONNX-–º–æ–¥–µ–ª—è–º
# wget https://huggingface.co/Xenova/all-MiniLM-L6-v2-onnx/resolve/main/model.onnx -O all-MiniLM-L6-v2.onnx
# all-MiniLM-L6-v2.onnx
# all-MiniLM-L6-v2_quantized.onnx
ONNX_MODELS = {
    "all-MiniLM-L6-v2": "models/all-MiniLM-L6-v2_quantized.onnx",
}

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
tokenizer = AutoTokenizer.from_pretrained("nixiesearch/all-MiniLM-L6-v2-onnx")


def get_embedding_onnx(onnx_session, text):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ ONNX-–º–æ–¥–µ–ª—å"""
    tokens = tokenizer(text, return_tensors="np", padding=True, truncation=True)

    input_feed = {
        "input_ids": tokens["input_ids"].astype(np.int64),
        "attention_mask": tokens["attention_mask"].astype(np.int64)
    }

    if "token_type_ids" in tokens:
        input_feed["token_type_ids"] = tokens["token_type_ids"].astype(np.int64)

    # –ó–∞–ø—É—Å–∫ ONNX-–º–æ–¥–µ–ª–∏
    outputs = onnx_session.run(None, input_feed)

    # üîç –û—Ç–ª–∞–¥–∫–∞: —Å–º–æ—Ç—Ä–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    print(f"üîé Model outputs: {[o.shape for o in outputs]}")

    embeddings = outputs[0]  # –≠—Ç–æ (1, N, 384), –≥–¥–µ N ‚Äî —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤
    embeddings = embeddings[0]  # –£–±–∏—Ä–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –±–∞—Ç—á–∞ (N, 384)

    # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º
    return embeddings.mean(axis=0)  # –¢–µ–ø–µ—Ä—å –≤—Å–µ–≥–¥–∞ (384,)


def cosine_similarity_score(vec1, vec2):
    """–†–∞—Å—á–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
    return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))


def test_model(model_name):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ONNX-–º–æ–¥–µ–ª–∏"""
    print(f"üîπ –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏: {model_name} [ONNX]")

    try:
        onnx_session = ort.InferenceSession(ONNX_MODELS[model_name], providers=["CPUExecutionProvider"])
        start_time = time.perf_counter()

        vec1 = get_embedding_onnx(onnx_session, TEXT1)
        vec2 = get_embedding_onnx(onnx_session, TEXT2)

        similarity = cosine_similarity_score(vec1, vec2)
        execution_time = time.perf_counter() - start_time

        print(f"  ‚úÖ –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f}")
        print(f"  ‚ö° –í—Ä–µ–º—è —Ä–∞—Å—á–µ—Ç–∞: {execution_time:.6f} —Å–µ–∫.\n")

    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}\n")


if __name__ == "__main__":
    # üìù –í—Ö–æ–¥–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    TEXT1 = "smallcat"
    TEXT2 = "small cat"

    print(f"üìå –§—Ä–∞–∑–∞ 1: {TEXT1}")
    print(f"üìå –§—Ä–∞–∑–∞ 2: {TEXT2}\n")

    # üöÄ –¢–µ—Å—Ç–∏—Ä—É–µ–º ONNX –º–æ–¥–µ–ª–∏
    for model in ONNX_MODELS:
        test_model(model)
