import time
import numpy as np
import onnxruntime as ort
from transformers import AutoTokenizer

# –ü—É—Ç–∏ –∫ ONNX-–º–æ–¥–µ–ª—è–º
# e5-small-v2.onnx                       FP32 - more precisely
# e5-small-v2_opt2_QInt8.onnx            INT8 - faster
ONNX_MODELS = {
    "e5-small-v2": "models/e5-small-v2_opt2_QInt8.onnx",  # –£–∫–∞–∂–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å
}

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
tokenizer = AutoTokenizer.from_pretrained("intfloat/e5-small-v2")


def get_embedding_onnx(onnx_session, text):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ ONNX-–º–æ–¥–µ–ª—å (e5-small-v2)"""
    # –î–æ–±–∞–≤–ª—è–µ–º "query: " –ø–µ—Ä–µ–¥ —Ç–µ–∫—Å—Ç–æ–º, –∫–∞–∫ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –∞–≤—Ç–æ—Ä –º–æ–¥–µ–ª–∏
    formatted_text = f"{text}"
    tokens = tokenizer(formatted_text, return_tensors="np", padding=True, truncation=True)

    input_feed = {
        "input_ids": tokens["input_ids"].astype(np.int64),
        "attention_mask": tokens["attention_mask"].astype(np.int64)
    }

    # üî• [–ò–°–ü–†–ê–í–õ–ï–ù–û]: –ù–µ –ø–µ—Ä–µ–¥–∞–µ–º `token_type_ids`, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if "token_type_ids" in tokens:
        input_feed["token_type_ids"] = tokens["token_type_ids"].astype(np.int64)

    # –ó–∞–ø—É—Å–∫ ONNX-–º–æ–¥–µ–ª–∏
    outputs = onnx_session.run(None, input_feed)

    # üîç –û—Ç–ª–∞–¥–∫–∞: —Å–º–æ—Ç—Ä–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    print(f"üîé Model outputs: {[o.shape for o in outputs]}")

    embeddings = outputs[0]  # –≠—Ç–æ (1, N, 256), –≥–¥–µ N ‚Äî —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤
    embeddings = embeddings[0]  # –£–±–∏—Ä–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –±–∞—Ç—á–∞ (N, 256)

    # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º
    return embeddings.mean(axis=0)  # –¢–µ–ø–µ—Ä—å –≤—Å–µ–≥–¥–∞ (256,)


def cosine_similarity_score(vec1, vec2):
    """–†–∞—Å—á–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
    return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))


def test_model(model_name):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ONNX-–º–æ–¥–µ–ª–∏"""
    print(f"üîπ –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏: {model_name} [ONNX]")

    try:
        onnx_session = ort.InferenceSession(ONNX_MODELS[model_name], providers=["CPUExecutionProvider"])
        start_time = time.perf_counter()

        vec1 = get_embedding_onnx(onnx_session, TEXT1)
        vec2 = get_embedding_onnx(onnx_session, TEXT2)

        similarity = cosine_similarity_score(vec1, vec2)
        execution_time = time.perf_counter() - start_time

        print(f"  ‚úÖ –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f}")
        print(f"  ‚ö° –í—Ä–µ–º—è —Ä–∞—Å—á–µ—Ç–∞: {execution_time:.6f} —Å–µ–∫.\n")

    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}\n")


if __name__ == "__main__":
    # üìù –í—Ö–æ–¥–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    TEXT1 = "bank card"
    TEXT2 = "dog"

    print(f"üìå –§—Ä–∞–∑–∞ 1: {TEXT1}")
    print(f"üìå –§—Ä–∞–∑–∞ 2: {TEXT2}\n")

    # üöÄ –¢–µ—Å—Ç–∏—Ä—É–µ–º ONNX –º–æ–¥–µ–ª—å
    for model in ONNX_MODELS:
        test_model(model)
